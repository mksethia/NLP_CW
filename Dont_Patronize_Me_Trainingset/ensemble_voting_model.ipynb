{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86574cff",
   "metadata": {},
   "source": [
    "# Ensemble Voting Model — Don't Patronize Me!\n",
    "\n",
    "**Binary PCL classification** using RoBERTa, DistilBERT, and DeBERTa with majority-vote ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fa19b",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563b2b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.10.0+cu128\n",
      "CUDA available  : True\n",
      "GPU device      : Tesla T4\n",
      "GPU memory      : 15.6 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device      : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory      : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a9ee6",
   "metadata": {},
   "source": [
    "## 2. Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929e9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device    : cuda\n",
      "bf16 supported  : True\n",
      "Class weights   : tensor([1., 9.], device='cuda:0')  (device: cuda:0)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_LABELS = 2\n",
    "LABEL_NAMES = [\"Non-PCL\", \"PCL\"]\n",
    "\n",
    "# Class weights for the ~9.5:1 imbalance (Non-PCL : PCL).\n",
    "# Placing on DEVICE once avoids repeated .to() calls inside compute_loss.\n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 9.0], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Mixed-precision strategy:\n",
    "#   bf16 preferred (Ampere+ GPUs) — works with all models including DeBERTa v3.\n",
    "#   fp16 as fallback for older GPUs — but NOT safe for DeBERTa v3 (FP16 gradient error).\n",
    "_BF16_OK = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "print(f\"Using device    : {DEVICE}\")\n",
    "print(f\"bf16 supported  : {_BF16_OK}\")\n",
    "print(f\"Class weights   : {CLASS_WEIGHTS}  (device: {CLASS_WEIGHTS.device})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df6c65",
   "metadata": {},
   "source": [
    "## 3. Load & Preprocess Dataset\n",
    "\n",
    "Binary labels as per the paper: labels 0-1 → **Non-PCL (0)**, labels 2-4 → **PCL (1)**.\n",
    "\n",
    "We split 80/10/10 into train / val / test. The test set is held out entirely until final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef6cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples  : 10468\n",
      "Label distribution:\n",
      "binary_label\n",
      "Non-PCL    9475\n",
      "PCL         993\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 9.54:1\n",
      "\n",
      "Split sizes — train: 8374, val: 1047, test: 1047\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load Don't Patronize Me PCL dataset and binarise labels.\"\"\"\n",
    "    pcl_columns = [\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"]\n",
    "    df = pd.read_csv(\n",
    "        \"dontpatronizeme_pcl.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        skiprows=4,\n",
    "        names=pcl_columns,\n",
    "        on_bad_lines=\"skip\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing text or labels\n",
    "    df = df.dropna(subset=[\"text\", \"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    # Binary: 0-1 → Non-PCL (0),  2-4 → PCL (1)\n",
    "    df[\"binary_label\"] = (df[\"label\"] >= 2).astype(int)\n",
    "\n",
    "    print(f\"Total samples  : {len(df)}\")\n",
    "    print(f\"Label distribution:\\n{df['binary_label'].value_counts().rename({0: 'Non-PCL', 1: 'PCL'})}\")\n",
    "    print(f\"Imbalance ratio: {(df['binary_label'] == 0).sum() / (df['binary_label'] == 1).sum():.2f}:1\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# 80 / 10 / 10 stratified split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"binary_label\"], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"binary_label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_df[\"text\"].tolist(), \"label\": train_df[\"binary_label\"].tolist()})\n",
    "val_dataset   = Dataset.from_dict({\"text\": val_df[\"text\"].tolist(),   \"label\": val_df[\"binary_label\"].tolist()})\n",
    "test_dataset  = Dataset.from_dict({\"text\": test_df[\"text\"].tolist(),  \"label\": test_df[\"binary_label\"].tolist()})\n",
    "\n",
    "print(f\"\\nSplit sizes — train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0aa4b3",
   "metadata": {},
   "source": [
    "## 4. Model Definitions & Tokenisation\n",
    "\n",
    "We define:\n",
    "- **Model catalogue** — three transformer architectures\n",
    "- **`WeightedTrainer`** — custom Trainer that uses class-weighted CrossEntropyLoss. The class weights tensor is moved to device **once** (at init), not on every forward pass.\n",
    "- **`compute_metrics`** — accuracy, precision, recall, F1\n",
    "- Per-model tokenisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bccf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokeniser for RoBERTa\n",
      "Loaded tokeniser for DistilBERT\n",
      "Loaded tokeniser for DeBERTa\n"
     ]
    }
   ],
   "source": [
    "MODEL_CATALOGUE = {\n",
    "    \"RoBERTa\":    \"FacebookAI/roberta-base\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"DeBERTa\":    \"microsoft/deberta-v3-base\",\n",
    "}\n",
    "\n",
    "MAX_LENGTH = 128  # EDA: median 42 word tokens, 95th pct ~105; subword inflation ~1.3x → 128 is safe\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mixed-precision helper — decides fp16 vs bf16 per model\n",
    "# ---------------------------------------------------------------------------\n",
    "def get_mixed_precision_flags(model_name: str):\n",
    "    \"\"\"Return (fp16, bf16) flags for a given model.\n",
    "\n",
    "    • bf16 is preferred for ALL models when the GPU supports it (Ampere+).\n",
    "    • fp16 is used as fallback — except for DeBERTa v3 which produces\n",
    "      gradient-unscale errors under fp16.\n",
    "    • DeBERTa v3 falls back to fp32 if bf16 is unavailable.\n",
    "    \"\"\"\n",
    "    if _BF16_OK:\n",
    "        return False, True          # bf16 for everything\n",
    "    if model_name == \"DeBERTa\":\n",
    "        return False, False         # fp32 fallback (fp16 is unsafe)\n",
    "    if torch.cuda.is_available():\n",
    "        return True, False          # fp16 for other models\n",
    "    return False, False             # CPU\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Weighted Trainer — class weights live on the same device as the model\n",
    "# ---------------------------------------------------------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Trainer that applies class weights to CrossEntropyLoss.\n",
    "\n",
    "    Supports per-trial class_weight_pos override via self.args (set by\n",
    "    Optuna hp search). Falls back to the weights passed at init.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # ---- NaN guard: clamp logits to prevent NaN propagation ----------\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "        # Use per-trial class_weight_pos if set by hp search, else default\n",
    "        pos_w = getattr(self.args, \"class_weight_pos\", None)\n",
    "        if pos_w is not None:\n",
    "            weights = torch.tensor([1.0, pos_w], dtype=logits.dtype, device=logits.device)\n",
    "        else:\n",
    "            weights = self._class_weights.to(dtype=logits.dtype, device=logits.device)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, F1 for the positive class (PCL).\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Tokenisers — NO padding here; DataCollatorWithPadding pads per-batch\n",
    "# (median text ~42 tokens → dynamic padding is ~2x faster than pad-to-128)\n",
    "# ---------------------------------------------------------------------------\n",
    "tokenisers = {}\n",
    "for name, path in MODEL_CATALOGUE.items():\n",
    "    tokenisers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    print(f\"Loaded tokeniser for {name}\")\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenise a HuggingFace Dataset with the given tokenizer (no padding).\"\"\"\n",
    "    def _tok(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], truncation=True, max_length=MAX_LENGTH\n",
    "        )\n",
    "    return dataset.map(_tok, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685a9f0",
   "metadata": {},
   "source": [
    "## 5. Bayesian Hyperparameter Optimisation (Optuna)\n",
    "\n",
    "For each model we run `trainer.hyperparameter_search` with an Optuna backend. This performs **Bayesian optimisation** (Tree-structured Parzen Estimator by default) over learning rate, number of epochs, batch size, weight decay, and **class weight for PCL** (searched 8–10 around the ~9.5:1 natural ratio).\n",
    "\n",
    "Key design decisions:\n",
    "- **`model_init`** function (not a pre-built model) so Trainer can reinitialise fresh weights each trial\n",
    "- **`class_weight_pos`** in the search space — the most impactful knob for imbalanced classification\n",
    "- **Dynamic padding** (`DataCollatorWithPadding`) — pads per-batch instead of to `MAX_LENGTH`, ~2× faster\n",
    "- **DeBERTa v3** uses `bf16` (or fp32 fallback) instead of `fp16` which causes gradient unscale errors\n",
    "- `direction=\"maximize\"` because we optimise F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f50ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Hyperparameter search for RoBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 17671.66 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 17312.91 examples/s]\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 983.69it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 16:44:53,352]\u001b[0m A new study created in memory with name: no-name-ef04eba7-7188-48db-b1bb-f09b8c8a2c38\u001b[0m\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 897.53it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678291</td>\n",
       "      <td>0.515019</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.475872</td>\n",
       "      <td>0.403342</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.432432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493968</td>\n",
       "      <td>0.412185</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.463768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:47:33,438]\u001b[0m Trial 0 finished with value: 0.463768115942029 and parameters: {'learning_rate': 1.9694467477810197e-05, 'weight_decay': 0.14430748653421396, 'class_weight_pos': 8.203403785315341}. Best is trial 0 with value: 0.463768115942029.\u001b[0m\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1007.42it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]             \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683338</td>\n",
       "      <td>0.602280</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.549210</td>\n",
       "      <td>0.406949</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.501658</td>\n",
       "      <td>0.404051</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.432432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:50:21,419]\u001b[0m Trial 1 finished with value: 0.43243243243243246 and parameters: {'learning_rate': 1.19864923657565e-05, 'weight_decay': 0.05715588276704438, 'class_weight_pos': 8.053462063594088}. Best is trial 0 with value: 0.463768115942029.\u001b[0m\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 995.01it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683094</td>\n",
       "      <td>0.511272</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.475813</td>\n",
       "      <td>0.417009</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.415584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512144</td>\n",
       "      <td>0.442178</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:53:11,361]\u001b[0m Trial 2 finished with value: 0.45714285714285713 and parameters: {'learning_rate': 1.947233740455189e-05, 'weight_decay': 0.10385095864858529, 'class_weight_pos': 9.76936002334696}. Best is trial 0 with value: 0.463768115942029.\u001b[0m\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1028.19it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]             \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/189 00:55 < 01:55, 1.09 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685113</td>\n",
       "      <td>0.596188</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:54:08,836]\u001b[0m Trial 3 pruned. \u001b[0m\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 969.77it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677769</td>\n",
       "      <td>0.489566</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462854</td>\n",
       "      <td>0.398484</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.441558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508907</td>\n",
       "      <td>0.418185</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.478873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:56:59,890]\u001b[0m Trial 4 finished with value: 0.4788732394366197 and parameters: {'learning_rate': 2.132434171951195e-05, 'weight_decay': 0.0798457371027612, 'class_weight_pos': 8.567344860075119}. Best is trial 4 with value: 0.4788732394366197.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ RoBERTa best trial — F1: 0.4789\n",
      "  Hyperparameters: {'learning_rate': 2.132434171951195e-05, 'weight_decay': 0.0798457371027612, 'class_weight_pos': 8.567344860075119}\n",
      "\n",
      "============================================================\n",
      "  Hyperparameter search for DistilBERT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 14526.97 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 14145.62 examples/s]\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 407.86it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 16:57:06,464]\u001b[0m A new study created in memory with name: no-name-f813ea45-0c11-495e-bca5-dc3a5ebf147a\u001b[0m\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1046.42it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686796</td>\n",
       "      <td>0.590645</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.459016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.577975</td>\n",
       "      <td>0.511890</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.329670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.518122</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.389610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 16:58:33,275]\u001b[0m Trial 0 finished with value: 0.38961038961038963 and parameters: {'learning_rate': 1.3905632831511787e-05, 'weight_decay': 0.03558678803511963, 'class_weight_pos': 9.172857141790868}. Best is trial 0 with value: 0.38961038961038963.\u001b[0m\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1055.48it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677527</td>\n",
       "      <td>0.508987</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.520150</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.336634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.513526</td>\n",
       "      <td>0.453968</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:00:00,265]\u001b[0m Trial 1 finished with value: 0.39473684210526316 and parameters: {'learning_rate': 2.1647883049730073e-05, 'weight_decay': 0.04910889965504239, 'class_weight_pos': 8.992252623188367}. Best is trial 1 with value: 0.39473684210526316.\u001b[0m\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 968.82it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/189 00:28 < 00:58, 2.14 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684345</td>\n",
       "      <td>0.568047</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:00:29,591]\u001b[0m Trial 2 pruned. \u001b[0m\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 999.90it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]    \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/189 00:28 < 00:58, 2.14 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680049</td>\n",
       "      <td>0.523997</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:00:58,997]\u001b[0m Trial 3 pruned. \u001b[0m\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 987.97it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/189 00:28 < 00:58, 2.15 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679452</td>\n",
       "      <td>0.519614</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:01:28,289]\u001b[0m Trial 4 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DistilBERT best trial — F1: 0.3947\n",
      "  Hyperparameters: {'learning_rate': 2.1647883049730073e-05, 'weight_decay': 0.04910889965504239, 'class_weight_pos': 8.992252623188367}\n",
      "\n",
      "============================================================\n",
      "  Hyperparameter search for DeBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 14984.97 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 14669.61 examples/s]\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1277.11it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 17:01:38,374]\u001b[0m A new study created in memory with name: no-name-621e4616-e373-4d95-b7d9-457d14630630\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1270.76it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.708392</td>\n",
       "      <td>0.645954</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.676645</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688829</td>\n",
       "      <td>0.680808</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:05:00,739]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'learning_rate': 1.023843533490562e-05, 'weight_decay': 0.09078497329645818, 'class_weight_pos': 8.609598584900937}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1172.06it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.753577</td>\n",
       "      <td>0.831927</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.027853</td>\n",
       "      <td>0.791786</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.747893</td>\n",
       "      <td>0.663471</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:08:22,474]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'learning_rate': 1.499614194477049e-05, 'weight_decay': 0.0540273395429711, 'class_weight_pos': 8.749417271870307}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1155.20it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751962</td>\n",
       "      <td>0.628898</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.732722</td>\n",
       "      <td>0.731243</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.707636</td>\n",
       "      <td>0.683043</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:11:44,128]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'learning_rate': 1.8963698406793235e-05, 'weight_decay': 0.130097146034042, 'class_weight_pos': 9.173895424547274}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1131.99it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.775009</td>\n",
       "      <td>0.639667</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.702790</td>\n",
       "      <td>0.779520</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.727684</td>\n",
       "      <td>0.685689</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:15:05,858]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'learning_rate': 1.8398322698148384e-05, 'weight_decay': 0.12387171609043818, 'class_weight_pos': 9.846986528438917}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1204.60it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.550920</td>\n",
       "      <td>1.204318</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.965424</td>\n",
       "      <td>0.776806</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.761134</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 17:18:28,113]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'learning_rate': 2.5231076504025018e-05, 'weight_decay': 0.17658090154738026, 'class_weight_pos': 8.303777365927857}. Best is trial 0 with value: 0.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DeBERTa best trial — F1: 0.0000\n",
      "  Hyperparameters: {'learning_rate': 1.023843533490562e-05, 'weight_decay': 0.09078497329645818, 'class_weight_pos': 8.609598584900937}\n"
     ]
    }
   ],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    \"\"\"Bayesian search over the two highest-impact hyperparameters.\n",
    "\n",
    "    Fixed with educated defaults (not worth searching):\n",
    "      • per_device_train_batch_size = 32  (marginal quality diff, 2× faster than 16)\n",
    "      • num_train_epochs = 3              (standard for transformer fine-tuning)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"learning_rate\":             trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True),\n",
    "        \"weight_decay\":              trial.suggest_float(\"weight_decay\", 0.0, 0.2),\n",
    "        \"class_weight_pos\":          trial.suggest_float(\"class_weight_pos\", 8.0, 10.0),\n",
    "    }\n",
    "\n",
    "\n",
    "N_TRIALS = 5  # 2-dimensional search → 5 trials gives good coverage\n",
    "\n",
    "best_hparams = {}   # {model_name: BestRun}\n",
    "trained_models = {} # {model_name: fine-tuned model}\n",
    "\n",
    "for name, model_path in MODEL_CATALOGUE.items():\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Hyperparameter search for {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    train_tok = tokenize_dataset(train_dataset, tokenizer)\n",
    "    val_tok   = tokenize_dataset(val_dataset, tokenizer)\n",
    "\n",
    "    # model_init: required so Trainer can create a fresh model each trial\n",
    "    def make_model_init(path):\n",
    "        def model_init():\n",
    "            return AutoModelForSequenceClassification.from_pretrained(\n",
    "                path, num_labels=NUM_LABELS\n",
    "            )\n",
    "        return model_init\n",
    "\n",
    "    # Mixed-precision: bf16 when available (all models); fp16 fallback (not DeBERTa)\n",
    "    use_fp16, use_bf16 = get_mixed_precision_flags(name)\n",
    "    print(f\"  Mixed precision — fp16: {use_fp16}, bf16: {use_bf16}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{name}\",\n",
    "        num_train_epochs=3,                # fixed — standard for fine-tuning\n",
    "        per_device_train_batch_size=32,    # fixed — faster than 16, similar quality\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",               # no checkpoints during HP search (saves disk)\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=50,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        warmup_ratio=0.1,\n",
    "        max_grad_norm=1.0,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    # Seed attribute so Optuna's setattr succeeds for class_weight_pos\n",
    "    training_args.class_weight_pos = 9.0\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=CLASS_WEIGHTS,\n",
    "        model_init=make_model_init(model_path),\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok.shuffle(seed=42).select(range(2000)),\n",
    "        eval_dataset=val_tok.shuffle(seed=42).select(range(250)),\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    best_run = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\",\n",
    "        backend=\"optuna\",\n",
    "        hp_space=optuna_hp_space,\n",
    "        n_trials=N_TRIALS,\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
    "        compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    )\n",
    "\n",
    "    best_hparams[name] = best_run\n",
    "    print(f\"\\n✓ {name} best trial — F1: {best_run.objective:.4f}\")\n",
    "    print(f\"  Hyperparameters: {best_run.hyperparameters}\")\n",
    "\n",
    "    # Free GPU memory before next model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1de03",
   "metadata": {},
   "source": [
    "## 6. Train Each Model with Best Hyperparameters\n",
    "\n",
    "Re-train each model from scratch using the best hyperparameters found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea15d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Final training: RoBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 18535.69 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 17352.31 examples/s]\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 792.43it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 12:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.479007</td>\n",
       "      <td>0.365185</td>\n",
       "      <td>0.846227</td>\n",
       "      <td>0.365639</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.507645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343715</td>\n",
       "      <td>0.342505</td>\n",
       "      <td>0.829990</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.502793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231397</td>\n",
       "      <td>0.439241</td>\n",
       "      <td>0.903534</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.600791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma', 'roberta.encoder.layer.6.attention.output.LayerNorm.beta', 'roberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.6.output.LayerNorm.beta', 'roberta.encoder.layer.6.output.LayerNorm.gamma', 'roberta.encoder.layer.7.attention.output.LayerNorm.beta', 'roberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.7.output.LayerNorm.beta', 'roberta.encoder.layer.7.output.LayerNorm.gamma', 'roberta.encoder.layer.8.attention.output.LayerNorm.beta', 'roberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.8.output.LayerNorm.beta', 'roberta.encoder.layer.8.output.LayerNorm.gamma', 'roberta.encoder.layer.9.attention.output.LayerNorm.beta', 'roberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.9.output.LayerNorm.beta', 'roberta.encoder.layer.9.output.LayerNorm.gamma', 'roberta.encoder.layer.10.attention.output.LayerNorm.beta', 'roberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.10.output.LayerNorm.beta', 'roberta.encoder.layer.10.output.LayerNorm.gamma', 'roberta.encoder.layer.11.attention.output.LayerNorm.beta', 'roberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.11.output.LayerNorm.beta', 'roberta.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.57\n",
      "✓ RoBERTa final training complete.\n",
      "\n",
      "============================================================\n",
      "  Final training: DistilBERT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 14123.43 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 13997.86 examples/s]\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1049.15it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 06:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463273</td>\n",
       "      <td>0.435884</td>\n",
       "      <td>0.805158</td>\n",
       "      <td>0.295276</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345962</td>\n",
       "      <td>0.392214</td>\n",
       "      <td>0.845272</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.496894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230847</td>\n",
       "      <td>0.481754</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.99\n",
      "✓ DistilBERT final training complete.\n",
      "\n",
      "============================================================\n",
      "  Final training: DeBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8374/8374 [00:00<00:00, 15311.30 examples/s]\n",
      "Map: 100%|██████████| 1047/1047 [00:00<00:00, 14695.73 examples/s]\n",
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 1175.93it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision — fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 14:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.227539</td>\n",
       "      <td>0.777258</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.812098</td>\n",
       "      <td>0.693162</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.756232</td>\n",
       "      <td>0.688421</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.LayerNorm.weight', 'deberta.encoder.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.beta', 'deberta.embeddings.LayerNorm.gamma', 'deberta.encoder.layer.0.attention.output.LayerNorm.beta', 'deberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.0.output.LayerNorm.beta', 'deberta.encoder.layer.0.output.LayerNorm.gamma', 'deberta.encoder.layer.1.attention.output.LayerNorm.beta', 'deberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.1.output.LayerNorm.beta', 'deberta.encoder.layer.1.output.LayerNorm.gamma', 'deberta.encoder.layer.2.attention.output.LayerNorm.beta', 'deberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.2.output.LayerNorm.beta', 'deberta.encoder.layer.2.output.LayerNorm.gamma', 'deberta.encoder.layer.3.attention.output.LayerNorm.beta', 'deberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.3.output.LayerNorm.beta', 'deberta.encoder.layer.3.output.LayerNorm.gamma', 'deberta.encoder.layer.4.attention.output.LayerNorm.beta', 'deberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.4.output.LayerNorm.beta', 'deberta.encoder.layer.4.output.LayerNorm.gamma', 'deberta.encoder.layer.5.attention.output.LayerNorm.beta', 'deberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.5.output.LayerNorm.beta', 'deberta.encoder.layer.5.output.LayerNorm.gamma', 'deberta.encoder.layer.6.attention.output.LayerNorm.beta', 'deberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.6.output.LayerNorm.beta', 'deberta.encoder.layer.6.output.LayerNorm.gamma', 'deberta.encoder.layer.7.attention.output.LayerNorm.beta', 'deberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.7.output.LayerNorm.beta', 'deberta.encoder.layer.7.output.LayerNorm.gamma', 'deberta.encoder.layer.8.attention.output.LayerNorm.beta', 'deberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.8.output.LayerNorm.beta', 'deberta.encoder.layer.8.output.LayerNorm.gamma', 'deberta.encoder.layer.9.attention.output.LayerNorm.beta', 'deberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.9.output.LayerNorm.beta', 'deberta.encoder.layer.9.output.LayerNorm.gamma', 'deberta.encoder.layer.10.attention.output.LayerNorm.beta', 'deberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.10.output.LayerNorm.beta', 'deberta.encoder.layer.10.output.LayerNorm.gamma', 'deberta.encoder.layer.11.attention.output.LayerNorm.beta', 'deberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.11.output.LayerNorm.beta', 'deberta.encoder.layer.11.output.LayerNorm.gamma', 'deberta.encoder.LayerNorm.beta', 'deberta.encoder.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.61\n",
      "✓ DeBERTa final training complete.\n"
     ]
    }
   ],
   "source": [
    "trainers = {}  # keep trainers around for prediction\n",
    "\n",
    "for name, model_path in MODEL_CATALOGUE.items():\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Final training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best = best_hparams[name]\n",
    "    hp = best.hyperparameters\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    train_tok = tokenize_dataset(train_dataset, tokenizer)\n",
    "    val_tok   = tokenize_dataset(val_dataset, tokenizer)\n",
    "\n",
    "    # Build fresh model with best HPs\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=NUM_LABELS\n",
    "    )\n",
    "\n",
    "    # Verify the model is on the expected device (Trainer will move it, but let's show it)\n",
    "    print(f\"  Model device before Trainer: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Mixed-precision: bf16 when available (all models); fp16 fallback (not DeBERTa)\n",
    "    use_fp16, use_bf16 = get_mixed_precision_flags(name)\n",
    "    print(f\"  Mixed precision — fp16: {use_fp16}, bf16: {use_bf16}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{name}_final\",\n",
    "        num_train_epochs=3,                                      # fixed\n",
    "        per_device_train_batch_size=32,                          # fixed\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=hp.get(\"learning_rate\", 2e-5),\n",
    "        weight_decay=hp.get(\"weight_decay\", 0.01),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=50,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        warmup_ratio=0.1,\n",
    "        max_grad_norm=1.0,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Apply best class weight from hp search\n",
    "    class_w = hp.get(\"class_weight_pos\", 9.0)\n",
    "    final_weights = torch.tensor([1.0, class_w], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=final_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # After training, Trainer has moved the model to GPU (if available)\n",
    "    print(f\"  Model device after Trainer : {next(model.parameters()).device}\")\n",
    "    print(f\"  Class weight (PCL)        : {class_w:.2f}\")\n",
    "\n",
    "    trained_models[name] = model\n",
    "    trainers[name] = trainer\n",
    "    print(f\"✓ {name} final training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e4119",
   "metadata": {},
   "source": [
    "## 7. Per-Model Evaluation — Results & Confusion Matrices\n",
    "\n",
    "Evaluate each model individually on the **test set**, print classification reports, and plot confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c40e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_model_preds = {}  # {name: np.array of predictions on test set}\n",
    "\n",
    "for name in MODEL_CATALOGUE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Test Evaluation: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    test_tok = tokenize_dataset(test_dataset, tokenizer)\n",
    "    trainer = trainers[name]\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = trainer.predict(test_tok)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "    per_model_preds[name] = preds\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\n{name} — Classification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=LABEL_NAMES).plot(\n",
    "        ax=axes[0], cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    axes[0].set_title(f\"{name} — Counts\")\n",
    "\n",
    "    cm_norm = confusion_matrix(labels, preds, normalize=\"true\")\n",
    "    ConfusionMatrixDisplay(cm_norm, display_labels=LABEL_NAMES).plot(\n",
    "        ax=axes[1], cmap=\"Blues\", colorbar=False, values_format=\".2%\"\n",
    "    )\n",
    "    axes[1].set_title(f\"{name} — Normalised\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e197d25",
   "metadata": {},
   "source": [
    "## 8. Overall Ensemble — Majority Vote, Results & Confusion Matrix\n",
    "\n",
    "Each of the 3 models votes; a sample is classified as **PCL** if **2 or more** models agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote: PCL (1) if >= 2 out of 3 models predict PCL\n",
    "votes = np.stack(list(per_model_preds.values()), axis=0)  # (3, n_test)\n",
    "ensemble_preds = (votes.sum(axis=0) >= 2).astype(int)\n",
    "true_labels = np.array(test_dataset[\"label\"])\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Overall classification report\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"  ENSEMBLE (Majority Vote) — Test Set Results\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_labels, ensemble_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Per-model vs ensemble summary table\n",
    "rows = []\n",
    "for name, preds in per_model_preds.items():\n",
    "    p, r, f1, _ = precision_recall_fscore_support(true_labels, preds, average=\"binary\", pos_label=1)\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    rows.append({\"Model\": name, \"Accuracy\": acc, \"Precision\": p, \"Recall\": r, \"F1\": f1})\n",
    "\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average=\"binary\", pos_label=1)\n",
    "acc = accuracy_score(true_labels, ensemble_preds)\n",
    "rows.append({\"Model\": \"ENSEMBLE\", \"Accuracy\": acc, \"Precision\": p, \"Recall\": r, \"F1\": f1})\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"Model\")\n",
    "print(\"\\nSummary comparison:\")\n",
    "display(summary_df.style.format(\"{:.4f}\").highlight_max(axis=0, color=\"lightgreen\"))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Confusion matrices — ensemble\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "cm = confusion_matrix(true_labels, ensemble_preds)\n",
    "ConfusionMatrixDisplay(cm, display_labels=LABEL_NAMES).plot(\n",
    "    ax=axes[0], cmap=\"Oranges\", colorbar=False\n",
    ")\n",
    "axes[0].set_title(\"Ensemble — Counts\")\n",
    "\n",
    "cm_norm = confusion_matrix(true_labels, ensemble_preds, normalize=\"true\")\n",
    "ConfusionMatrixDisplay(cm_norm, display_labels=LABEL_NAMES).plot(\n",
    "    ax=axes[1], cmap=\"Oranges\", colorbar=False, values_format=\".2%\"\n",
    ")\n",
    "axes[1].set_title(\"Ensemble — Normalised\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Voting agreement heatmap\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nPer-sample voting agreement:\")\n",
    "agreement = votes.sum(axis=0)\n",
    "for v in [0, 1, 2, 3]:\n",
    "    count = (agreement == v).sum()\n",
    "    print(f\"  {v}/3 models predict PCL: {count} samples ({count/len(agreement)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
