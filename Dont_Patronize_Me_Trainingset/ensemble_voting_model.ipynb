{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86574cff",
   "metadata": {},
   "source": [
    "# Ensemble Voting Model ‚Äî Don't Patronize Me!\n",
    "\n",
    "**Binary PCL classification** using RoBERTa, DistilBERT, and DeBERTa with majority-vote ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fa19b",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563b2b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.10.0+cu128\n",
      "CUDA available  : True\n",
      "GPU device      : Tesla T4\n",
      "GPU memory      : 15.6 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device      : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory      : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a9ee6",
   "metadata": {},
   "source": [
    "## 2. Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929e9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device    : cuda\n",
      "bf16 supported  : True\n",
      "Class weights   : tensor([1., 9.], device='cuda:0')  (device: cuda:0)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_LABELS = 2\n",
    "LABEL_NAMES = [\"Non-PCL\", \"PCL\"]\n",
    "\n",
    "# Class weights for the ~9.5:1 imbalance (Non-PCL : PCL).\n",
    "# Placing on DEVICE once avoids repeated .to() calls inside compute_loss.\n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 9.0], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Mixed-precision strategy:\n",
    "#   bf16 preferred (Ampere+ GPUs) ‚Äî works with all models including DeBERTa v3.\n",
    "#   fp16 as fallback for older GPUs ‚Äî but NOT safe for DeBERTa v3 (FP16 gradient error).\n",
    "_BF16_OK = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "print(f\"Using device    : {DEVICE}\")\n",
    "print(f\"bf16 supported  : {_BF16_OK}\")\n",
    "print(f\"Class weights   : {CLASS_WEIGHTS}  (device: {CLASS_WEIGHTS.device})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df6c65",
   "metadata": {},
   "source": [
    "## 3. Load & Preprocess Dataset\n",
    "\n",
    "Binary labels as per the paper: labels 0-1 ‚Üí **Non-PCL (0)**, labels 2-4 ‚Üí **PCL (1)**.\n",
    "\n",
    "We split 80/10/10 into train / val / test. The test set is held out entirely until final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef6cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples  : 10468\n",
      "Label distribution:\n",
      "binary_label\n",
      "Non-PCL    9475\n",
      "PCL         993\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 9.54:1\n",
      "\n",
      "Split sizes ‚Äî train: 8374, val: 1047, test: 1047\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load Don't Patronize Me PCL dataset and binarise labels.\"\"\"\n",
    "    pcl_columns = [\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"]\n",
    "    df = pd.read_csv(\n",
    "        \"dontpatronizeme_pcl.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        skiprows=4,\n",
    "        names=pcl_columns,\n",
    "        on_bad_lines=\"skip\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing text or labels\n",
    "    df = df.dropna(subset=[\"text\", \"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    # Binary: 0-1 ‚Üí Non-PCL (0),  2-4 ‚Üí PCL (1)\n",
    "    df[\"binary_label\"] = (df[\"label\"] >= 2).astype(int)\n",
    "\n",
    "    print(f\"Total samples  : {len(df)}\")\n",
    "    print(f\"Label distribution:\\n{df['binary_label'].value_counts().rename({0: 'Non-PCL', 1: 'PCL'})}\")\n",
    "    print(f\"Imbalance ratio: {(df['binary_label'] == 0).sum() / (df['binary_label'] == 1).sum():.2f}:1\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# 80 / 10 / 10 stratified split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"binary_label\"], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"binary_label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_df[\"text\"].tolist(), \"label\": train_df[\"binary_label\"].tolist()})\n",
    "val_dataset   = Dataset.from_dict({\"text\": val_df[\"text\"].tolist(),   \"label\": val_df[\"binary_label\"].tolist()})\n",
    "test_dataset  = Dataset.from_dict({\"text\": test_df[\"text\"].tolist(),  \"label\": test_df[\"binary_label\"].tolist()})\n",
    "\n",
    "print(f\"\\nSplit sizes ‚Äî train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0aa4b3",
   "metadata": {},
   "source": [
    "## 4. Model Definitions & Tokenisation\n",
    "\n",
    "We define:\n",
    "- **Model catalogue** ‚Äî three transformer architectures\n",
    "- **`WeightedTrainer`** ‚Äî custom Trainer that uses class-weighted CrossEntropyLoss. The class weights tensor is moved to device **once** (at init), not on every forward pass.\n",
    "- **`compute_metrics`** ‚Äî accuracy, precision, recall, F1\n",
    "- Per-model tokenisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bccf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokeniser for RoBERTa\n",
      "Loaded tokeniser for DistilBERT\n",
      "Loaded tokeniser for DeBERTa\n"
     ]
    }
   ],
   "source": [
    "MODEL_CATALOGUE = {\n",
    "    \"RoBERTa\":    \"FacebookAI/roberta-base\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"DeBERTa\":    \"microsoft/deberta-v3-base\",\n",
    "}\n",
    "\n",
    "MAX_LENGTH = 128  # EDA: median 42 word tokens, 95th pct ~105; subword inflation ~1.3x ‚Üí 128 is safe\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mixed-precision helper ‚Äî decides fp16 vs bf16 per model\n",
    "# ---------------------------------------------------------------------------\n",
    "def get_mixed_precision_flags(model_name: str):\n",
    "    \"\"\"Return (fp16, bf16) flags for a given model.\n",
    "\n",
    "    ‚Ä¢ bf16 is preferred for ALL models when the GPU supports it (Ampere+).\n",
    "    ‚Ä¢ fp16 is used as fallback ‚Äî except for DeBERTa v3 which produces\n",
    "      gradient-unscale errors under fp16.\n",
    "    ‚Ä¢ DeBERTa v3 falls back to fp32 if bf16 is unavailable.\n",
    "    \"\"\"\n",
    "    if _BF16_OK:\n",
    "        return False, True          # bf16 for everything\n",
    "    if model_name == \"DeBERTa\":\n",
    "        return False, False         # fp32 fallback (fp16 is unsafe)\n",
    "    if torch.cuda.is_available():\n",
    "        return True, False          # fp16 for other models\n",
    "    return False, False             # CPU\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Weighted Trainer ‚Äî class weights live on the same device as the model\n",
    "# ---------------------------------------------------------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Trainer that applies class weights to CrossEntropyLoss.\n",
    "\n",
    "    Supports per-trial class_weight_pos override via self.args (set by\n",
    "    Optuna hp search). Falls back to the weights passed at init.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # ---- NaN guard: clamp logits to prevent NaN propagation ----------\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "        # Use per-trial class_weight_pos if set by hp search, else default\n",
    "        pos_w = getattr(self.args, \"class_weight_pos\", None)\n",
    "        if pos_w is not None:\n",
    "            weights = torch.tensor([1.0, pos_w], dtype=logits.dtype, device=logits.device)\n",
    "        else:\n",
    "            weights = self._class_weights.to(dtype=logits.dtype, device=logits.device)\n",
    "            \n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights, reduction=\"mean\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, F1 for the positive class (PCL).\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Tokenisers ‚Äî NO padding here; DataCollatorWithPadding pads per-batch\n",
    "# (median text ~42 tokens ‚Üí dynamic padding is ~2x faster than pad-to-128)\n",
    "# ---------------------------------------------------------------------------\n",
    "tokenisers = {}\n",
    "for name, path in MODEL_CATALOGUE.items():\n",
    "    tokenisers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    print(f\"Loaded tokeniser for {name}\")\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenise a HuggingFace Dataset with the given tokenizer (no padding).\"\"\"\n",
    "    def _tok(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], truncation=True, max_length=MAX_LENGTH\n",
    "        )\n",
    "    return dataset.map(_tok, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685a9f0",
   "metadata": {},
   "source": [
    "## 5. Bayesian Hyperparameter Optimisation (Optuna)\n",
    "\n",
    "For each model we run `trainer.hyperparameter_search` with an Optuna backend. This performs **Bayesian optimisation** (Tree-structured Parzen Estimator by default) over learning rate, number of epochs, batch size, weight decay, and **class weight for PCL** (searched 8‚Äì10 around the ~9.5:1 natural ratio).\n",
    "\n",
    "Key design decisions:\n",
    "- **`model_init`** function (not a pre-built model) so Trainer can reinitialise fresh weights each trial\n",
    "- **`class_weight_pos`** in the search space ‚Äî the most impactful knob for imbalanced classification\n",
    "- **Dynamic padding** (`DataCollatorWithPadding`) ‚Äî pads per-batch instead of to `MAX_LENGTH`, ~2√ó faster\n",
    "- **DeBERTa v3** uses `bf16` (or fp32 fallback) instead of `fp16` which causes gradient unscale errors\n",
    "- `direction=\"maximize\"` because we optimise F1\n",
    "\n",
    "Set `USE_PREVIOUS_HPARAMS` per-model in the cell below to **load saved results** from `best_hparams.json` or **re-run** the Optuna search (which overwrites the file on completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb197ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RoBERTa      ‚Üí üìÇ load saved\n",
      "  DistilBERT   ‚Üí üìÇ load saved\n",
      "  DeBERTa      ‚Üí üîç search\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Toggle: reuse saved hyperparameters or re-run Optuna search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Set per-model: True  = load from best_hparams.json (fast, no GPU needed)\n",
    "#                False = run Bayesian HP search with Optuna (overwrites file)\n",
    "\n",
    "HPARAMS_FILE = Path(\"best_hparams.json\")\n",
    "\n",
    "USE_PREVIOUS_HPARAMS = {\n",
    "    \"RoBERTa\":    True,\n",
    "    \"DistilBERT\": True,\n",
    "    \"DeBERTa\":    False,\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ Quick sanity check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "_need_file = any(USE_PREVIOUS_HPARAMS.values())\n",
    "if _need_file and not HPARAMS_FILE.exists():\n",
    "    print(f\"‚ö†  {HPARAMS_FILE} not found ‚Äî will run search for ALL models.\")\n",
    "    USE_PREVIOUS_HPARAMS = {k: False for k in USE_PREVIOUS_HPARAMS}\n",
    "else:\n",
    "    for name, reuse in USE_PREVIOUS_HPARAMS.items():\n",
    "        tag = \"üìÇ load saved\" if reuse else \"üîç search\"\n",
    "        print(f\"  {name:12s} ‚Üí {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f50ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved hyperparameters from best_hparams.json\n",
      "  ‚ö†  Adding default batch_size=32 to RoBERTa (was not in saved hparams)\n",
      "\n",
      "============================================================\n",
      "  RoBERTa: loaded saved hyperparameters (F1: 0.4789)\n",
      "  {'learning_rate': 2.132434171951195e-05, 'weight_decay': 0.0798457371027612, 'class_weight_pos': 8.567344860075119, 'batch_size': 32}\n",
      "============================================================\n",
      "  ‚ö†  Adding default batch_size=32 to DistilBERT (was not in saved hparams)\n",
      "\n",
      "============================================================\n",
      "  DistilBERT: loaded saved hyperparameters (F1: 0.3947)\n",
      "  {'learning_rate': 2.1647883049730073e-05, 'weight_decay': 0.04910889965504239, 'class_weight_pos': 8.992252623188367, 'batch_size': 32}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "  Hyperparameter search for DeBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [00:00<00:00, 14842.32 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1047/1047 [00:00<00:00, 14015.28 examples/s]\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mixed precision ‚Äî fp16: False, bf16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:00<00:00, 1100.36it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "\u001b[32m[I 2026-02-11 00:03:09,198]\u001b[0m A new study created in memory with name: no-name-5e17a752-d4ae-41f7-975b-c92f9ade7af1\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:00<00:00, 1133.60it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/189 01:57 < 00:51, 1.11 it/s, Epoch 2.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.809933</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.076878</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-02-11 00:05:08,499]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 2.664834358379492e-05, 'weight_decay': 0.11320959206192294, 'class_weight_pos': 9.318440908476063, 'batch_size': 32} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py\", line 253, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2170, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2537, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 3838, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2852, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 630, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 364, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/azureuser/nlp/.venv/lib/python3.12/site-packages/torch/autograd/graph.py\", line 865, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-02-11 00:05:08,501]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    105\u001b[39m training_args.class_weight_pos = \u001b[32m9.0\u001b[39m\n\u001b[32m    107\u001b[39m trainer = WeightedTrainer(\n\u001b[32m    108\u001b[39m     class_weights=CLASS_WEIGHTS,\n\u001b[32m    109\u001b[39m     model_init=make_model_init(model_path),\n\u001b[32m   (...)\u001b[39m\u001b[32m    115\u001b[39m     data_collator=DataCollatorWithPadding(tokenizer),\n\u001b[32m    116\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m best_run = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptuna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhp_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptuna_hp_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpruner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMedianPruner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_startup_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_warmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_objective\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_f1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m best_hparams[name] = best_run\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m best trial ‚Äî F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_run.objective\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:3554\u001b[39m, in \u001b[36mTrainer.hyperparameter_search\u001b[39m\u001b[34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[39m\n\u001b[32m   3551\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = hp_name\n\u001b[32m   3552\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_objective = default_compute_objective \u001b[38;5;28;01mif\u001b[39;00m compute_objective \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m compute_objective\n\u001b[32m-> \u001b[39m\u001b[32m3554\u001b[39m best_run = \u001b[43mbackend_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3556\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_search_backend = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_run\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/hyperparameter_search.py:68\u001b[39m, in \u001b[36mOptunaBackend.run\u001b[39m\u001b[34m(self, trainer, n_trials, direction, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, n_trials: \u001b[38;5;28mint\u001b[39m, direction: \u001b[38;5;28mstr\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_hp_search_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:272\u001b[39m, in \u001b[36mrun_hp_search_optuna\u001b[39m\u001b[34m(trainer, n_trials, direction, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m direction = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m directions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m direction\n\u001b[32m    271\u001b[39m study = optuna.create_study(direction=direction, directions=directions, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m study._is_multi_objective():\n\u001b[32m    276\u001b[39m     best_trial = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:68\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:165\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:263\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    259\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    262\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:206\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    209\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:253\u001b[39m, in \u001b[36mrun_hp_search_optuna.<locals>._objective\u001b[39m\u001b[34m(trial, checkpoint_dir)\u001b[39m\n\u001b[32m    251\u001b[39m     trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(trainer, \u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:2170\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2168\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2171\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:2537\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2530\u001b[39m context = (\n\u001b[32m   2531\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2532\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2533\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2535\u001b[39m )\n\u001b[32m   2536\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2537\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2540\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2541\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2542\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2543\u001b[39m ):\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2545\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:3838\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3836\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    \"\"\"Bayesian search over the highest-impact hyperparameters.\n",
    "\n",
    "    Searched hyperparameters:\n",
    "      ‚Ä¢ learning_rate\n",
    "      ‚Ä¢ weight_decay\n",
    "      ‚Ä¢ class_weight_pos (for imbalanced classification)\n",
    "      ‚Ä¢ batch_size (16 or 32)\n",
    "\n",
    "    Fixed with educated defaults (not worth searching):\n",
    "      ‚Ä¢ num_train_epochs = 3              (standard for transformer fine-tuning)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"learning_rate\":             trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"weight_decay\":              trial.suggest_float(\"weight_decay\", 0.0, 0.2),\n",
    "        \"class_weight_pos\":          trial.suggest_float(\"class_weight_pos\", 9.0, 10.0),\n",
    "        \"batch_size\":                trial.suggest_categorical(\"batch_size\", [16, 32]),\n",
    "    }\n",
    "\n",
    "\n",
    "N_TRIALS = 5  # 4-dimensional search ‚Üí 5 trials gives good coverage\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load any previously saved hyperparameters\n",
    "# ---------------------------------------------------------------------------\n",
    "saved_hparams = {}\n",
    "if HPARAMS_FILE.exists():\n",
    "    with open(HPARAMS_FILE) as f:\n",
    "        saved_hparams = json.load(f)\n",
    "    print(f\"Loaded saved hyperparameters from {HPARAMS_FILE}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Run search (or load) for each model\n",
    "# ---------------------------------------------------------------------------\n",
    "best_hparams = {}   # {model_name: BestRun}\n",
    "trained_models = {} # {model_name: fine-tuned model}\n",
    "\n",
    "for name, model_path in MODEL_CATALOGUE.items():\n",
    "\n",
    "    # ‚îÄ‚îÄ Use previously saved hyperparameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if USE_PREVIOUS_HPARAMS.get(name, False):\n",
    "        if name in saved_hparams:\n",
    "            hp = saved_hparams[name]\n",
    "            hparams_dict = hp[\"hyperparameters\"].copy()\n",
    "            \n",
    "            # Backward compatibility: add default batch_size if missing\n",
    "            if \"batch_size\" not in hparams_dict:\n",
    "                hparams_dict[\"batch_size\"] = 32\n",
    "                print(f\"  ‚ö†  Adding default batch_size=32 to {name} (was not in saved hparams)\")\n",
    "            \n",
    "            # Reconstruct a BestRun-compatible namedtuple\n",
    "            from transformers.trainer_utils import BestRun\n",
    "            best_hparams[name] = BestRun(\n",
    "                run_id=\"saved\",\n",
    "                objective=hp[\"objective\"],\n",
    "                hyperparameters=hparams_dict,\n",
    "            )\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"  {name}: loaded saved hyperparameters (F1: {hp['objective']:.4f})\")\n",
    "            print(f\"  {hparams_dict}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"\\n‚ö†  {name} not found in {HPARAMS_FILE} ‚Äî running search instead.\")\n",
    "\n",
    "    # ‚îÄ‚îÄ Run Optuna search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Hyperparameter search for {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    train_tok = tokenize_dataset(train_dataset, tokenizer)\n",
    "    val_tok   = tokenize_dataset(val_dataset, tokenizer)\n",
    "\n",
    "    # model_init: required so Trainer can create a fresh model each trial\n",
    "    def make_model_init(path):\n",
    "        def model_init():\n",
    "            return AutoModelForSequenceClassification.from_pretrained(\n",
    "                path, num_labels=NUM_LABELS\n",
    "            )\n",
    "        return model_init\n",
    "\n",
    "    # Mixed-precision: bf16 when available (all models); fp16 fallback (not DeBERTa)\n",
    "    use_fp16, use_bf16 = get_mixed_precision_flags(name)\n",
    "    print(f\"  Mixed precision ‚Äî fp16: {use_fp16}, bf16: {use_bf16}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{name}\",\n",
    "        num_train_epochs=3,                # fixed ‚Äî standard for fine-tuning\n",
    "        per_device_train_batch_size=32,    # initial value; will be overridden by hp search\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",               # no checkpoints during HP search (saves disk)\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=50,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        warmup_ratio=0.1,\n",
    "        max_grad_norm=1.0,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    # Seed attributes so Optuna's setattr succeeds for hp search\n",
    "    training_args.batch_size = 32\n",
    "    training_args.class_weight_pos = 9.0\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=CLASS_WEIGHTS,\n",
    "        model_init=make_model_init(model_path),\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok.shuffle(seed=42).select(range(2000)),\n",
    "        eval_dataset=val_tok.shuffle(seed=42).select(range(250)),\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    best_run = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\",\n",
    "        backend=\"optuna\",\n",
    "        hp_space=optuna_hp_space,\n",
    "        n_trials=N_TRIALS,\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
    "        compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    )\n",
    "\n",
    "    best_hparams[name] = best_run\n",
    "    print(f\"\\n‚úì {name} best trial ‚Äî F1: {best_run.objective:.4f}\")\n",
    "    print(f\"  Hyperparameters: {best_run.hyperparameters}\")\n",
    "\n",
    "    # Free GPU memory before next model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Save ALL best hyperparameters back to file (merge with existing)\n",
    "# ---------------------------------------------------------------------------\n",
    "for name, run in best_hparams.items():\n",
    "    saved_hparams[name] = {\n",
    "        \"objective\": run.objective,\n",
    "        \"hyperparameters\": run.hyperparameters,\n",
    "    }\n",
    "\n",
    "with open(HPARAMS_FILE, \"w\") as f:\n",
    "    json.dump(saved_hparams, f, indent=4)\n",
    "print(f\"\\n‚úì Best hyperparameters saved to {HPARAMS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1de03",
   "metadata": {},
   "source": [
    "## 6. Train Each Model with Best Hyperparameters\n",
    "\n",
    "Re-train each model from scratch using the best hyperparameters found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Final training: RoBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [00:00<00:00, 18535.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1047/1047 [00:00<00:00, 17352.31 examples/s]\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 792.43it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision ‚Äî fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 12:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.479007</td>\n",
       "      <td>0.365185</td>\n",
       "      <td>0.846227</td>\n",
       "      <td>0.365639</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.507645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343715</td>\n",
       "      <td>0.342505</td>\n",
       "      <td>0.829990</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.502793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231397</td>\n",
       "      <td>0.439241</td>\n",
       "      <td>0.903534</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.600791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.77it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma', 'roberta.encoder.layer.6.attention.output.LayerNorm.beta', 'roberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.6.output.LayerNorm.beta', 'roberta.encoder.layer.6.output.LayerNorm.gamma', 'roberta.encoder.layer.7.attention.output.LayerNorm.beta', 'roberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.7.output.LayerNorm.beta', 'roberta.encoder.layer.7.output.LayerNorm.gamma', 'roberta.encoder.layer.8.attention.output.LayerNorm.beta', 'roberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.8.output.LayerNorm.beta', 'roberta.encoder.layer.8.output.LayerNorm.gamma', 'roberta.encoder.layer.9.attention.output.LayerNorm.beta', 'roberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.9.output.LayerNorm.beta', 'roberta.encoder.layer.9.output.LayerNorm.gamma', 'roberta.encoder.layer.10.attention.output.LayerNorm.beta', 'roberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.10.output.LayerNorm.beta', 'roberta.encoder.layer.10.output.LayerNorm.gamma', 'roberta.encoder.layer.11.attention.output.LayerNorm.beta', 'roberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.11.output.LayerNorm.beta', 'roberta.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.57\n",
      "‚úì RoBERTa final training complete.\n",
      "\n",
      "============================================================\n",
      "  Final training: DistilBERT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [00:00<00:00, 14123.43 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1047/1047 [00:00<00:00, 13997.86 examples/s]\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 1049.15it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision ‚Äî fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 06:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463273</td>\n",
       "      <td>0.435884</td>\n",
       "      <td>0.805158</td>\n",
       "      <td>0.295276</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345962</td>\n",
       "      <td>0.392214</td>\n",
       "      <td>0.845272</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.496894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230847</td>\n",
       "      <td>0.481754</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.99\n",
      "‚úì DistilBERT final training complete.\n",
      "\n",
      "============================================================\n",
      "  Final training: DeBERTa\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [00:00<00:00, 15311.30 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1047/1047 [00:00<00:00, 14695.73 examples/s]\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:00<00:00, 1175.93it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device before Trainer: cpu\n",
      "  Mixed precision ‚Äî fp16: False, bf16: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [786/786 14:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.227539</td>\n",
       "      <td>0.777258</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.812098</td>\n",
       "      <td>0.693162</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.756232</td>\n",
       "      <td>0.688421</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.37it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.LayerNorm.weight', 'deberta.encoder.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.beta', 'deberta.embeddings.LayerNorm.gamma', 'deberta.encoder.layer.0.attention.output.LayerNorm.beta', 'deberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.0.output.LayerNorm.beta', 'deberta.encoder.layer.0.output.LayerNorm.gamma', 'deberta.encoder.layer.1.attention.output.LayerNorm.beta', 'deberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.1.output.LayerNorm.beta', 'deberta.encoder.layer.1.output.LayerNorm.gamma', 'deberta.encoder.layer.2.attention.output.LayerNorm.beta', 'deberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.2.output.LayerNorm.beta', 'deberta.encoder.layer.2.output.LayerNorm.gamma', 'deberta.encoder.layer.3.attention.output.LayerNorm.beta', 'deberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.3.output.LayerNorm.beta', 'deberta.encoder.layer.3.output.LayerNorm.gamma', 'deberta.encoder.layer.4.attention.output.LayerNorm.beta', 'deberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.4.output.LayerNorm.beta', 'deberta.encoder.layer.4.output.LayerNorm.gamma', 'deberta.encoder.layer.5.attention.output.LayerNorm.beta', 'deberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.5.output.LayerNorm.beta', 'deberta.encoder.layer.5.output.LayerNorm.gamma', 'deberta.encoder.layer.6.attention.output.LayerNorm.beta', 'deberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.6.output.LayerNorm.beta', 'deberta.encoder.layer.6.output.LayerNorm.gamma', 'deberta.encoder.layer.7.attention.output.LayerNorm.beta', 'deberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.7.output.LayerNorm.beta', 'deberta.encoder.layer.7.output.LayerNorm.gamma', 'deberta.encoder.layer.8.attention.output.LayerNorm.beta', 'deberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.8.output.LayerNorm.beta', 'deberta.encoder.layer.8.output.LayerNorm.gamma', 'deberta.encoder.layer.9.attention.output.LayerNorm.beta', 'deberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.9.output.LayerNorm.beta', 'deberta.encoder.layer.9.output.LayerNorm.gamma', 'deberta.encoder.layer.10.attention.output.LayerNorm.beta', 'deberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.10.output.LayerNorm.beta', 'deberta.encoder.layer.10.output.LayerNorm.gamma', 'deberta.encoder.layer.11.attention.output.LayerNorm.beta', 'deberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.11.output.LayerNorm.beta', 'deberta.encoder.layer.11.output.LayerNorm.gamma', 'deberta.encoder.LayerNorm.beta', 'deberta.encoder.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model device after Trainer : cuda:0\n",
      "  Class weight (PCL)        : 8.61\n",
      "‚úì DeBERTa final training complete.\n"
     ]
    }
   ],
   "source": [
    "trainers = {}  # keep trainers around for prediction\n",
    "\n",
    "for name, model_path in MODEL_CATALOGUE.items():\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Final training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best = best_hparams[name]\n",
    "    hp = best.hyperparameters\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    train_tok = tokenize_dataset(train_dataset, tokenizer)\n",
    "    val_tok   = tokenize_dataset(val_dataset, tokenizer)\n",
    "\n",
    "    # Build fresh model with best HPs\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=NUM_LABELS\n",
    "    )\n",
    "\n",
    "    # Verify the model is on the expected device (Trainer will move it, but let's show it)\n",
    "    print(f\"  Model device before Trainer: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Mixed-precision: bf16 when available (all models); fp16 fallback (not DeBERTa)\n",
    "    use_fp16, use_bf16 = get_mixed_precision_flags(name)\n",
    "    print(f\"  Mixed precision ‚Äî fp16: {use_fp16}, bf16: {use_bf16}\")\n",
    "\n",
    "    # Apply best batch size from hp search (backward compatible: default to 32)\n",
    "    batch_size = hp.get(\"batch_size\", 32)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{name}_final\",\n",
    "        num_train_epochs=3,                                      # fixed\n",
    "        per_device_train_batch_size=batch_size,                  # from hp search\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=hp.get(\"learning_rate\", 2e-5),\n",
    "        weight_decay=hp.get(\"weight_decay\", 0.01),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=50,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        warmup_ratio=0.1,\n",
    "        max_grad_norm=1.0,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Apply best class weight from hp search\n",
    "    class_w = hp.get(\"class_weight_pos\", 9.0)\n",
    "    final_weights = torch.tensor([1.0, class_w], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    print(f\"  Batch size (train & eval)  : {batch_size}\")\n",
    "    print(f\"  Class weight (PCL)        : {class_w:.2f}\")\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=final_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # After training, Trainer has moved the model to GPU (if available)\n",
    "    print(f\"  Model device after Trainer : {next(model.parameters()).device}\")\n",
    "\n",
    "    trained_models[name] = model\n",
    "    trainers[name] = trainer\n",
    "    print(f\"‚úì {name} final training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e4119",
   "metadata": {},
   "source": [
    "## 7. Per-Model Evaluation ‚Äî Results & Confusion Matrices\n",
    "\n",
    "Evaluate each model individually on the **test set**, print classification reports, and plot confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c40e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_model_preds = {}  # {name: np.array of predictions on test set}\n",
    "\n",
    "for name in MODEL_CATALOGUE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Test Evaluation: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    tokenizer = tokenisers[name]\n",
    "    test_tok = tokenize_dataset(test_dataset, tokenizer)\n",
    "    trainer = trainers[name]\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = trainer.predict(test_tok)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "    per_model_preds[name] = preds\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\n{name} ‚Äî Classification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=LABEL_NAMES).plot(\n",
    "        ax=axes[0], cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    axes[0].set_title(f\"{name} ‚Äî Counts\")\n",
    "\n",
    "    cm_norm = confusion_matrix(labels, preds, normalize=\"true\")\n",
    "    ConfusionMatrixDisplay(cm_norm, display_labels=LABEL_NAMES).plot(\n",
    "        ax=axes[1], cmap=\"Blues\", colorbar=False, values_format=\".2%\"\n",
    "    )\n",
    "    axes[1].set_title(f\"{name} ‚Äî Normalised\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e197d25",
   "metadata": {},
   "source": [
    "## 8. Overall Ensemble ‚Äî Majority Vote, Results & Confusion Matrix\n",
    "\n",
    "Each of the 3 models votes; a sample is classified as **PCL** if **2 or more** models agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote: PCL (1) if >= 2 out of 3 models predict PCL\n",
    "votes = np.stack(list(per_model_preds.values()), axis=0)  # (3, n_test)\n",
    "ensemble_preds = (votes.sum(axis=0) >= 2).astype(int)\n",
    "true_labels = np.array(test_dataset[\"label\"])\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Overall classification report\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"  ENSEMBLE (Majority Vote) ‚Äî Test Set Results\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_labels, ensemble_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Per-model vs ensemble summary table\n",
    "rows = []\n",
    "for name, preds in per_model_preds.items():\n",
    "    p, r, f1, _ = precision_recall_fscore_support(true_labels, preds, average=\"binary\", pos_label=1)\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    rows.append({\"Model\": name, \"Accuracy\": acc, \"Precision\": p, \"Recall\": r, \"F1\": f1})\n",
    "\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average=\"binary\", pos_label=1)\n",
    "acc = accuracy_score(true_labels, ensemble_preds)\n",
    "rows.append({\"Model\": \"ENSEMBLE\", \"Accuracy\": acc, \"Precision\": p, \"Recall\": r, \"F1\": f1})\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"Model\")\n",
    "print(\"\\nSummary comparison:\")\n",
    "display(summary_df.style.format(\"{:.4f}\").highlight_max(axis=0, color=\"lightgreen\"))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Confusion matrices ‚Äî ensemble\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "cm = confusion_matrix(true_labels, ensemble_preds)\n",
    "ConfusionMatrixDisplay(cm, display_labels=LABEL_NAMES).plot(\n",
    "    ax=axes[0], cmap=\"Oranges\", colorbar=False\n",
    ")\n",
    "axes[0].set_title(\"Ensemble ‚Äî Counts\")\n",
    "\n",
    "cm_norm = confusion_matrix(true_labels, ensemble_preds, normalize=\"true\")\n",
    "ConfusionMatrixDisplay(cm_norm, display_labels=LABEL_NAMES).plot(\n",
    "    ax=axes[1], cmap=\"Oranges\", colorbar=False, values_format=\".2%\"\n",
    ")\n",
    "axes[1].set_title(\"Ensemble ‚Äî Normalised\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Voting agreement heatmap\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nPer-sample voting agreement:\")\n",
    "agreement = votes.sum(axis=0)\n",
    "for v in [0, 1, 2, 3]:\n",
    "    count = (agreement == v).sum()\n",
    "    print(f\"  {v}/3 models predict PCL: {count} samples ({count/len(agreement)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
